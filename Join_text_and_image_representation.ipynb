{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Join text and image representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow from tensorflow.keras.models\n",
    "import Sequential, load_model\n",
    "import numpy as np from tensorflow.keras.applications.resnet50\n",
    "import ResNet50 from tensorflow.keras.applications.resnet50 \n",
    "import preprocess_input,␣ ‹→decode_predictions from tensorflow.keras.preprocessing\n",
    "import image from tensorflow.keras.applications.imagenet_utils \n",
    "import preprocess_input  resnet_model = ResNet50(weights='imagenet' , include_top= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) x =\n",
    "    image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0) x =\n",
    "    preprocess_input(x) features = resnet_model.predict(x)\n",
    "    return np.expand_dims(features.flatten(), axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('resnet50-features.10k.npy')\n",
    "print(features.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Text Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(captions_filename, features_filename):\n",
    "    features = np.load(features_filename) images = [] texts = []\n",
    "    with open(captions_filename) as fp: \n",
    "        for line in fp: tokens = line.strip().split()\n",
    "            images.append(tokens[0]) texts.append(' '.join(tokens[1:]))\n",
    "            return features, images, texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature , images , texts = load( \"annotations.10k.txt\", \"resnet50-features.10k. ‹→npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) captions = pad_sequences(sequences, maxlen=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "vocab['<eos>'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json with open('vocab.json', 'w') as fp: # save the vocab\n",
    "    fp.write(json.dumps(vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 load the pretrained embedded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding \n",
    "embedding_weights = embedding.load(vocab, 100,'glove.twitter.27B.100d.filtered. ‹→txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 putting together the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, GRU\n",
    "image_input = Input(shape=(2048,))\n",
    "caption_input = Input(shape=(16,))\n",
    "noise_input = Input(shape=(16,))\n",
    "caption_embedding = Embedding(len(vocab), 100,)\n",
    "input_length=16,weights=[embedding_weights]\n",
    "caption_rnn = GRU(256)\n",
    "image_dense = Dense(256, activation='tanh') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 creation of pipe line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipeline = image_dense(image_input)\n",
    "caption_pipeline = caption_rnn(caption_embedding(caption_input))\n",
    "noise_pipeline = caption_rnn(caption_embedding(noise_input)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 compute the dot product between image and caption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge positive_pair = merge.dot([image_pipeline, caption_pipeline] , axes=1,normalize=False ) \n",
    "negative_pair = merge.dot([image_pipeline, noise_pipeline], axes=1,normalize=False)\n",
    "output = merge.concatenate([positive_pair, negative_pair] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pair , negative_pair,output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 create multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "training_model = Model(inputs=[image_input, caption_input, noise_input],outputs=output)\n",
    "image_model = Model(inputs=image_input, outputs=image_pipeline) \n",
    "caption_model = Model(inputs=caption_input,outputs=caption_pipeline) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def custom_loss(y_true, y_pred): positive = y_pred[:,0] negative = y_pred[:,1] \n",
    "    return K.sum(K.maximum(0., 1. - positive + negative)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred): positive = y_pred[:,0] negative = y_pred[:,1] \n",
    "    return K.mean(positive > negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.compile(loss=custom_loss, optimizer='adam',metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 spiting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.copy(captions)\n",
    "fake_labels = np.zeros((len(features), 1))\n",
    "X_train = ( [features[:9000], captions[:9000], noise[:9000]] ) \n",
    "Y_train = ( fake_labels[:9000])\n",
    "X_valid = [features[-1000:], captions[-1000:], noise[-1000:]]\n",
    "Y_valid = fake_labels[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual trainig \n",
    "for epoch in range(10): np.random.shuffle(noise) # don’t forget to shuffle mismatched captions\n",
    "    training_model.fit(X_train, Y_train,validation_data=[X_valid, Y_valid],epochs =1,batch_size=64)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 saving model and representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.save('model.image') caption_model.save('model.caption') \n",
    " \n",
    "np.save('caption-representations',caption_model.predict(captions))\n",
    "np.save('imagerepresentations', image_model.predict(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Captioning novel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "image_model = load_model('model.image',compile=False) \n",
    "caption_model = load_model('model.caption',compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 load representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "caption_representations = np.load('caption-representations.npy')\n",
    "image_representations = np.load('image-representations.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "vocab = json.loads(open('vocab.json').read())\n",
    "def preprocess_texts(texts): output = [] \n",
    "    for text in texts: output.append([vocab[word]\n",
    "         if word in vocab \n",
    "            else 0 for word in text.split()])\n",
    "    return pad_sequences(output, maxlen=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_filename, n=10):\n",
    "    # generate image representation \n",
    "    for new image image_representation = image_model.predict(extract_features(image_filename)) # compute score of all captions in the dataset\n",
    "    scores = np.dot(caption_representations,image_representation.T).flatten() # compute indices of n best captions \n",
    "    indices = np.argpartition(scores, -n)[-n:] \n",
    "    indices = indices[np.argsort(scores[indices])] # display them \n",
    "    for i in [int(x) for x in reversed(indices)]:\n",
    "        print(scores[i], texts[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 searching for images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image(caption, n=10):\n",
    "    caption_representation = caption_model.predict(preprocess_texts([caption]))\n",
    "    scores = np.dot(image_representations, caption_representation.T).flatten() \n",
    "    indices = np.argpartition(scores, -n)[-n:] \n",
    "    indices = indices[np.argsort(scores[indices])] \n",
    "    for i in [int(x) for x in reversed(indices)]:\n",
    "        print(scores[i], images[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_image('a man in the snow on some skis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
